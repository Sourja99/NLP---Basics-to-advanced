{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In today's technologically advanced era I know that everyone must have come accross this word \"Machine Learning & AI\", Right âœ‹? And many interested would have some ideas as well. In this practical Handbook we will guide you through out the process of NLP."
      ],
      "metadata": {
        "id": "8kF3CtUqUVKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK (Natural Language Toolkit)** in a Python is a collection of libraries and modules which helps us in the entire process of Natural Language Processing"
      ],
      "metadata": {
        "id": "5rUwZQB2Tfkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Started with NLTK - Tokenizing Words and Sentences\n"
      ],
      "metadata": {
        "id": "EFqypwvzhbAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twrLBXYl-JP5",
        "outputId": "b579bb74-cb2f-4788-c54f-d8ef07167ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeVnVJr687zl"
      },
      "outputs": [],
      "source": [
        "my_text = \"'I am a Third Year Engineering Student from Vishwakarma Institute of Technology, Pune. I was born in Kolkata, India. I am a huge fan of Machine Learning and Artificial Intelligence. I have been working towards it from my First Year in College. I have completed two internships and am currently working as an Intern in Suvidha Foundations. I want to improve myself as a MLOPs and a Data Scientists. I know its quite hard, but I am confident that I will surely succeed'\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Xbn9GL7a87zn",
        "outputId": "c24ddbb7-9997-48ce-a28e-1564f0577073"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'I am a Third Year Engineering Student from Vishwakarma Institute of Technology, Pune. I was born in Kolkata, India. I am a huge fan of Machine Learning and Artificial Intelligence. I have been working towards it from my First Year in College. I have completed two internships and am currently working as an Intern in Suvidha Foundations. I want to improve myself as a MLOPs and a Data Scientists. I know its quite hard, but I am confident that I will surely succeed'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "my_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWf0nOqm87zo"
      },
      "source": [
        "*It's crucial to notice here that the computer interprets text bodies as a single string object in this instance. Therefore, we must figure out a way to divide this single body of text so that the computer may treat each word as a separate string object. This introduces the idea of word tokenization and sentence tokenization, which are both processes for dividing a single string object into tokens that each represent words or characters.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFyaU5yg87zp"
      },
      "source": [
        "#### Natural Language Toolkit (NLTK) module .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RezpFl187zq"
      },
      "source": [
        "\n",
        "#Comming back to our example, letâ€™s tokenize the sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfdWHCSK87zr"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXErLwf_87zr"
      },
      "source": [
        "#### Sentence Tokenize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPRpkpP087zr"
      },
      "outputs": [],
      "source": [
        "sample_sentence_tokens = sent_tokenize(my_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPzbqnJ87zu",
        "outputId": "f1e8f36c-e1af-4187-d5ae-3e407ad0de09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'I am a Third Year Engineering Student from Vishwakarma Institute of Technology, Pune.\", 'I was born in Kolkata, India.', 'I am a huge fan of Machine Learning and Artificial Intelligence.', 'I have been working towards it from my First Year in College.', 'I have completed two internships and am currently working as an Intern in Suvidha Foundations.', 'I want to improve myself as a MLOPs and a Data Scientists.', \"I know its quite hard, but I am confident that I will surely succeed'\"]\n"
          ]
        }
      ],
      "source": [
        "print(sample_sentence_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZoH25Zu87zu"
      },
      "source": [
        "#### Word Tokenize() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIoEQ4R487zv"
      },
      "outputs": [],
      "source": [
        "sample_word_tokens = word_tokenize(my_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIrsVFRg87zv",
        "outputId": "4c411c70-2c9e-406d-a90e-0e5c85b6e785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", 'I', 'am', 'a', 'Third', 'Year', 'Engineering', 'Student', 'from', 'Vishwakarma', 'Institute', 'of', 'Technology', ',', 'Pune', '.', 'I', 'was', 'born', 'in', 'Kolkata', ',', 'India', '.', 'I', 'am', 'a', 'huge', 'fan', 'of', 'Machine', 'Learning', 'and', 'Artificial', 'Intelligence', '.', 'I', 'have', 'been', 'working', 'towards', 'it', 'from', 'my', 'First', 'Year', 'in', 'College', '.', 'I', 'have', 'completed', 'two', 'internships', 'and', 'am', 'currently', 'working', 'as', 'an', 'Intern', 'in', 'Suvidha', 'Foundations', '.', 'I', 'want', 'to', 'improve', 'myself', 'as', 'a', 'MLOPs', 'and', 'a', 'Data', 'Scientists', '.', 'I', 'know', 'its', 'quite', 'hard', ',', 'but', 'I', 'am', 'confident', 'that', 'I', 'will', 'surely', 'succeed', \"'\"]\n"
          ]
        }
      ],
      "source": [
        "print(sample_word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zk6XpP687zv"
      },
      "source": [
        "#### What we have - Results Tokens ðŸ”¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0tgG1rI87zw"
      },
      "source": [
        "####We can now preprocess individual tokens that we have! From this point on, we can remove some of the unnecessary text from which we would not want to extract features.\n",
        "#### We split the text sentence/paragraph into a list of words. This completes the Segmentation and Tokenization Part "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we5K1jdq87zw"
      },
      "source": [
        "\n",
        "\n",
        "#Comming to our next step in NLP -Stop Words"
      ]
    }
  ]
}